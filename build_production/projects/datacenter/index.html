<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="canonical" href="/projects/datacenter">
    <meta name="description" content="Website description.">
    <script src="//unpkg.com/alpinejs" defer></script>
    <title>We Have Datacenter at Home</title>
    <script src="https://kit.fontawesome.com/6faa1ef8ce.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="/assets/build/css/main.css?id=5d2b23395a68b61dd62762010ea4965c">
    <script defer src="/assets/build/js/main.js?id=87e20e630f0ef874f012ef2d57511382"></script>
</head>

<body class="text-gray-900 py-10 antialiased">
    <div class="flex w-full h-auto items-center justify-center">
        <a href="/">
            <img class="mx-auto mb-4" src="/assets/img/xbazzi_ascii.png" alt="">
        </a>
    </div>
    <div class="max-w-4xl mx-auto">
    <div class="rounded-br-md rounded-bl-md  shadow-md shadow-purple-800">
    <div
        class="border-4 border-black justify-between items-center py-1 px-1 text-xl text-white 
                title text-lg bg-gradient-to-tr from-gray-800 via-gray-700 to-gray-800
    ">
        <div class="flex justify-between gap-2 items-center pl-1">
            <div class="flex gap-3 items-center">
                <span class="fa-solid fa-terminal"></span>
                <p class="text-white">We Have Datacenter at Home</p>
            </div>
            <span class="fa-solid fa-lightbulb"></span>
        </div>

    </div>
    <div class="p-1 text-verde rounded-br-md rounded-bl-md 
                bg-gradient-to-b from-black/90 via-gray-900/90 to-black/90">
        <p class="font-bold text-xsm pl-4 pt-4 text-verde text-xs">Written by Xander Bazzi on 11-20-23</p>
        <div class="my-4 mx-auto prose prose-invert prose-img:max-w-lg prose-img:mx-auto">
            <h1>We Have Datacenter at Home</h1>

<p>Digital technology is ubiquitous, and most of our daily life depends on it in some way.
Inevitably, this rapid digitalization of our world comes with the side effect of producing more data; and with it, the need to manage it. With the advent of Salesforce SaaS in the 90s, AWS in the early 2000s, and Azure in the past decade, it seems that the public cloud offered a viable solution to the data managament problem. However, as of recent, companies have been unmarrying from the cloud, with up to <a href="https://journal.uptimeinstitute.com/high-costs-drive-cloud-repatriation-but-impact-is-overstated/">33% percent of companies</a> returning to their on-premises roots in 2022.</p>

<p>Motivated by the wave of businesses egressing from the cloud, and empowered by my DevOps expertise, I decided to take control of my data by self-hosting some infrastructure. The rack itself is my take on the <a href="https://wiki.eth0.nl/index.php/LackRack">LackRack</a>. Check this bad boy out:</p>

<p><img src="/assets/img/infra_new.JPG" alt="Datacenter Rack" /></p>

<p>From top to bottom: Workstation, patch panel, 24x 1Gbps + 4x 10Gbps Brocade ICX 6450, 8x 10Gbps SFP+ Mikrotik CRS309, 4x Lenovo USSF PCs, custom-built Supermicro server, PSU. Three of the Tiny PCs are running in a Proxmox VE cluster, running AlmaLinux VMs, which in turn run containers for some of my apps; the fourth Tiny PC is my OPNsense firewall. The staple of the infrastructure -- and the main reason I started self-hosting -- is the Supermicro server running TrueNAS SCALE. In addition to providing high-availability, multi-level caching, redundancy, and backups, it also runs several containers through the iX Apps market. Every device is fitted with a 10Gbps NIC and connected to the Mikrotik switch, with most ports set in trunk mode to allow any host/VM to join any VLAN. You might be wondering how I managed to fit 10Gbps cards in those tiny PCs. Well, it wasn't easy:</p>

<p><img src="/assets/img/dc2.JPG" alt="Tiny with 10gbps" /></p>

<p>Why even bother with 10Gbps? Is it just a frivolous attempt at flaunting how fast I can transfer big files across the LAN? Yes, but the main determinant factor for rationalizing the bigger bandwidth is centralized block storage for the VMs. To truly have a functional high availability system, we need the state to be stored in a separate host so that any VM can immediately become operational when spooled up, minimizing downtime in case of another VM crashing. Moreover, 10Gbps is a necessity to be able to access these block storage devices with similar throughput to local drives. Take the SATA III standard: the maximum theoretical bandwith is 6Gbps, so I know I can at least have networked block storage as fast as a local SATA drive. In TrueNAS, the most commonly and recently accessed data is cached by SSDs, and then by RAM. The NAS is running on DDR4, which has a maximum theoretical bandwidth of ~170GBps = 1360Gbps, which means the bottleneck is definitely the 10Gbps network link. Here's the proof, as reported by the Mikrotik switch during a backup job:</p>

<p><img src="/assets/img/bandwidth.png" alt="Backups go brrr" /></p>
        </div>
    </div>
</div>
</div>
    <!-- Cloudflare Web Analytics -->
    <script defer src='https://static.cloudflareinsights.com/beacon.min.js'
        data-cf-beacon='{"token": "851787a3fc454e6e83a8ad922bcfe266"}'></script><!-- End Cloudflare Web Analytics -->
</body>

</html>
